{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pickle\nimport functools\nimport sys\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data.dataset import Dataset\nimport torch.nn.functional as F\nimport torchtext\nimport tqdm\nimport random\nimport nltk\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom nltk.lm import MLE\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nnltk.download('punkt')\nimport re\nimport os\nimport re\nimport string\nimport json\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nfrom bs4 import BeautifulSoup\nimport transformers\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertTokenizer, AutoTokenizer, BertModel, BertConfig, AutoModel, AdamW\nimport warnings","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-15T15:40:42.339083Z","iopub.execute_input":"2023-01-15T15:40:42.339992Z","iopub.status.idle":"2023-01-15T15:40:55.206045Z","shell.execute_reply.started":"2023-01-15T15:40:42.339849Z","shell.execute_reply":"2023-01-15T15:40:55.204788Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-01-15T15:40:55.207816Z","iopub.execute_input":"2023-01-15T15:40:55.208447Z","iopub.status.idle":"2023-01-15T15:41:08.878518Z","shell.execute_reply.started":"2023-01-15T15:40:55.208402Z","shell.execute_reply":"2023-01-15T15:41:08.877217Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.13.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (22.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.13)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"class BERTtestDataset(Dataset):\n    def posaddedtext(self,text):\n        \n        try:\n            doc = nlp(text)\n            for token in doc:\n                text = text + \" \" + token.tag_\n            return text\n        except:\n            return text\n    \n    def neraddedtext(self,text):\n        try:\n            doc = nlp(text)\n            for ent in doc.ents:\n                text = text + \" \" + ent.label_\n            return text\n        except:\n            return text\n    \n    def __init__(self, df, tokenizer, max_len):\n        self.df = df\n        self.max_len = max_len\n        self.text = df.Tweet\n        self.tokenizer = tokenizer\n       \n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        text = self.posaddedtext(self.text[index])\n#         text = self.text[index]\n        # text = self.neraddedtext(self.text[index])\n        inputs = self.tokenizer.encode_plus(\n            text,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n    \n        }","metadata":{"execution":{"iopub.status.busy":"2023-01-15T15:41:08.880563Z","iopub.execute_input":"2023-01-15T15:41:08.880959Z","iopub.status.idle":"2023-01-15T15:41:08.896089Z","shell.execute_reply.started":"2023-01-15T15:41:08.880912Z","shell.execute_reply":"2023-01-15T15:41:08.894434Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# To find out the current mood of the person who is making the statement out of the following label\nf=['anger', 'disgust', 'fear', 'joy', 'love','sadness']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice = \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2023-01-15T15:41:08.899826Z","iopub.execute_input":"2023-01-15T15:41:08.900330Z","iopub.status.idle":"2023-01-15T15:41:08.912256Z","shell.execute_reply.started":"2023-01-15T15:41:08.900293Z","shell.execute_reply":"2023-01-15T15:41:08.911089Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Defining some key variables that will be used later on in the training\nMAX_LEN = 80\n# TRAIN_BATCH_SIZE = 64\nVALID_BATCH_SIZE = 1\nEPOCHS = 5\nLEARNING_RATE = 2e-5\ntokenizer = AutoTokenizer.from_pretrained('roberta-large')","metadata":{"execution":{"iopub.status.busy":"2023-01-15T15:41:08.913776Z","iopub.execute_input":"2023-01-15T15:41:08.914366Z","iopub.status.idle":"2023-01-15T15:41:14.981024Z","shell.execute_reply.started":"2023-01-15T15:41:08.914332Z","shell.execute_reply":"2023-01-15T15:41:14.979822Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a82de78322446349b55bf1dcb96eba9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a89be0d2b43d4f02b5317cb0264b28ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"effe9e8934994ea6b9ebee8c328aa943"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0320f8203d1f4472802f443c2f1118a5"}},"metadata":{}}]},{"cell_type":"code","source":"# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n\nclass BERTClass(torch.nn.Module):\n    def __init__(self):\n        super(BERTClass, self).__init__()\n        self.roberta1 = AutoModel.from_pretrained('roberta-large')\n#         self.l2 = torch.nn.Dropout(0.3)\n        self.fc = torch.nn.Linear(1024,6)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, features1 = self.roberta1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n        # _, features2 = self.roberta2(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict = False)\n        # last_hidden_states = torch.cat([features1,features2],dim=1)\n        output = self.fc(features1)\n        return output\n\nmodel = BERTClass()\nmodel.to(device);","metadata":{"execution":{"iopub.status.busy":"2023-01-15T15:41:14.982970Z","iopub.execute_input":"2023-01-15T15:41:14.984127Z","iopub.status.idle":"2023-01-15T15:42:24.145261Z","shell.execute_reply.started":"2023-01-15T15:41:14.984052Z","shell.execute_reply":"2023-01-15T15:42:24.143539Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffe23af9115043c3abe5516499dd74a4"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"PATH = \"/kaggle/input/okkkkk/last_model (3).h5\"\nloaded_model = BERTClass()\nloaded_model.load_state_dict(torch.load(PATH,map_location=torch.device('cpu')))\n","metadata":{"execution":{"iopub.status.busy":"2023-01-15T15:45:41.197976Z","iopub.execute_input":"2023-01-15T15:45:41.198426Z","iopub.status.idle":"2023-01-15T15:45:59.140447Z","shell.execute_reply.started":"2023-01-15T15:45:41.198390Z","shell.execute_reply":"2023-01-15T15:45:59.139486Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":" p = loaded_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-01-15T15:46:02.147506Z","iopub.execute_input":"2023-01-15T15:46:02.148362Z","iopub.status.idle":"2023-01-15T15:46:02.178758Z","shell.execute_reply.started":"2023-01-15T15:46:02.148322Z","shell.execute_reply":"2023-01-15T15:46:02.177464Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"x = \"I am pleased to hear about you\"\narr = []\narr.append(x)\npf = pd.DataFrame()\npf['Tweet'] = arr\n\nvalid_dataset = BERTtestDataset(pf, tokenizer, MAX_LEN)\n\nvalid_loader = DataLoader(valid_dataset, batch_size=1, \n                           num_workers=2, shuffle=False, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-15T15:48:37.111165Z","iopub.execute_input":"2023-01-15T15:48:37.111543Z","iopub.status.idle":"2023-01-15T15:48:37.123251Z","shell.execute_reply.started":"2023-01-15T15:48:37.111513Z","shell.execute_reply":"2023-01-15T15:48:37.122058Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def validation():\n    loaded_model.eval()\n    fin_targets=[]\n    fin_outputs=[]\n    with torch.no_grad():\n        print(valid_loader)\n        for _, data in enumerate(valid_loader, 0):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n            outputs = loaded_model(ids, mask, token_type_ids)\n            \n            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n    return fin_outputs\n","metadata":{"execution":{"iopub.status.busy":"2023-01-15T15:48:38.714222Z","iopub.execute_input":"2023-01-15T15:48:38.715124Z","iopub.status.idle":"2023-01-15T15:48:38.722911Z","shell.execute_reply.started":"2023-01-15T15:48:38.715074Z","shell.execute_reply":"2023-01-15T15:48:38.721726Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"outputs = validation()\noutputs","metadata":{"execution":{"iopub.status.busy":"2023-01-15T15:48:39.171527Z","iopub.execute_input":"2023-01-15T15:48:39.171947Z","iopub.status.idle":"2023-01-15T15:48:40.191815Z","shell.execute_reply.started":"2023-01-15T15:48:39.171916Z","shell.execute_reply":"2023-01-15T15:48:40.190318Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"<torch.utils.data.dataloader.DataLoader object at 0x7fb57b0b4990>\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"[[0.996228814125061,\n  0.9213868379592896,\n  0.007203541696071625,\n  0.00035084792762063444,\n  0.0017582604195922613,\n  0.02842453494668007]]"},"metadata":{}}]},{"cell_type":"code","source":"x1 = outputs[0].index(max(outputs[0]))","metadata":{"execution":{"iopub.status.busy":"2023-01-15T15:48:40.194388Z","iopub.execute_input":"2023-01-15T15:48:40.194988Z","iopub.status.idle":"2023-01-15T15:48:40.202149Z","shell.execute_reply.started":"2023-01-15T15:48:40.194946Z","shell.execute_reply":"2023-01-15T15:48:40.201026Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# To display the current mood the person who is making the above statement\nf[x1]","metadata":{"execution":{"iopub.status.busy":"2023-01-15T15:48:40.203741Z","iopub.execute_input":"2023-01-15T15:48:40.204128Z","iopub.status.idle":"2023-01-15T15:48:40.218357Z","shell.execute_reply.started":"2023-01-15T15:48:40.204071Z","shell.execute_reply":"2023-01-15T15:48:40.216936Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"'anger'"},"metadata":{}}]}]}